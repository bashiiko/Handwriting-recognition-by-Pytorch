\documentclass[a4paper, 12pt, dvipdfmx]{jarticle}

\usepackage{graphicx}
\usepackage{type1cm}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{multirow}
\usepackage[top=30truemm,bottom=30truemm,left=25truemm,right=25truemm]{geometry}

%調整
\def\baselinestretch{0.75}
% 図と図の間のスペース
\setlength\floatsep{1truemm}
% 本文と図の間のスペース
\setlength\textfloatsep{1.2truemm}
% 本文中の図のスペース
\setlength\intextsep{8pt}
% 図とキャプションの間のスペース
\setlength\abovecaptionskip{0.5truemm}
%式と本文上
\setlength{\abovedisplayskip}{0.5pt} % 上部のマージン
%式と本文下
\setlength{\belowdisplayskip}{0.5pt} % 下部のマージン
\renewcommand{\arraystretch}{1.3}

\title{データマイニング課題\\ 手書き文字の認識}
\author{1930047 小林菜穂子}
\date{}

\begin{document}
\maketitle

\section{ネットワーク構造の変遷}
\subsection{中間層を増やす}
中間層を3層→6層に変化
\begin{table}[htb]
  \centering
  \caption{中間層の変化}
  \begin{tabular}{c|c} \hline
    中間層の数 & 認識精度 \\ \hline
    3 & 9529/10000 (95\%) \\
    6 & 9488/10000 (94\%) \\ \hline
  \end{tabular}
\end{table}

\subsection{最適化手法の変更，およびweight decayの設定}
最適化手法をAdamとMomentumSGDで比較．
また，weight decayの付加による変化を観察．
\begin{table}[htb]
  \centering
  \caption{中間層の変更}
  \begin{tabular}{c c |c} \hline
    最適化手法 & weight decay & 認識精度 \\ \hline
    Adam & 0 & 9488/10000 (94\%)  \\
    Adam & 0.001 & 9076/10000 (90\%) \\
    MomentumSGD & 0 & 9657/10000 (96\%)\\
    MomentumSGD & 0.001 & 9619/10000 (96\%) \\ \hline
  \end{tabular}
\end{table}

\subsection{畳み込みニューラルネットワーク}
畳み込み層2層＋全結合層2層に変更．最適化手法はMomentumSGDを採用し，weight decay=0.001とした．

\begin{table}[htb]
  \centering
  \caption{畳み込みニューラルネットワーク}
  \begin{tabular}{c|c} \hline
    ニューラルネットワークの種類　& 認識精度 \\ \hline
    順伝播型ニューラルネットワーク & 9619/10000 (96\%) \\
    畳み込みニューラルネットワーク & 9813/10000 (98\%) \\ \hline
  \end{tabular}
\end{table}

\subsection{Dropoutの実装}
畳み込み層のみ，全結合層のみ，両方に実装した場合で精度を比較する．Dropoutのユニットの選出確率は0.1とした．
\begin{table}[htb]
  \centering
  \caption{畳み込みニューラルネットワーク}
  \begin{tabular}{cccc|c} \hline
    \multicolumn{4}{c}{各階層におけるDropout}　& \multirow{2}{*}{認識精度} \\
    Conv1 & Conv2 & FC1 & FC2 & \\ \hline
    〇 & ✕ & ✕ & ✕ & 948/10000 (9\%) \\
    ✕ & 〇 & ✕ & ✕ & 962/10000 (9\%) \\
    〇 & 〇 & ✕ & ✕ & 1207/10000 (12\%) \\
    ✕ & ✕ & 〇 & ✕ & 847/10000 (8\%) \\
    ✕ & ✕ & ✕ & 〇 &  1141/10000 (11\%) \\
    ✕ & ✕ & 〇 & 〇 &  192/10000 (1\%) \\
    〇 & 〇 & 〇 & 〇 & 1249/10000 (12\%)\\ \hline
  \end{tabular}
\end{table}

\subsection{学習率の変更}
学習率を，0.01から0.001,0.1に変更し，精度を比較する．畳み込みニューラルネットワークを用い，最適化手法はMomentumSGDを採用した．weight decay=0.001．Dropoutは実装しない．

\begin{table}[htb]
  \centering
  \caption{学習率の変更}
  \begin{tabular}{c|c} \hline
    学習率　& 認識精度 \\ \hline
    0.001 & 9437/10000 (94\%) \\
    0.01 & 9813/10000 (98\%) \\
    0.1 & 9231/10000 (92\%) \\ \hline
  \end{tabular}
\end{table}

\subsection{Optunaを用いたハイパーパラメータのチューニング}
Dropoutは実装していない．
\begin{itemize}
  \item 活性化関数：ReLU or ELU
  \item 最適化手法：Adam or MomentumSGD
  \item 学習率：$1.0\times 10^{-5}$～$1.0\times 10^{-1}$
  \item weight decayの設定：$1.0\times 10^{-10}$～$1.0\times 10^{-3}$
\end{itemize}
Optunaで100回試行し，得られた最適なパラメータは以下の通り．

\begin{table}[htb]
  \centering
  \caption{最適化されたパラメータ}
  \begin{tabular}{c|c} \hline
    活性化関数 & ReLU\\
    最適化手法 & MomentumSGD \\
    学習率 & 0.048160721856100486 \\
    weight decay & 2.940744863502711 $\times 10^{-7}$ \\ \hline
  \end{tabular}
\end{table}
また，表６のパラメータを用いた認識精度は\bf{99\%}であった．


\section{ネットワークの意図}
LeNetに近い構造を持つ，畳み込み層2層と全結合層2層から構成された，畳み込みニューラルネットワークを用いる．畳み込みニューラルネットワークは一般的な順伝播型のニューラルネットワークとは異なり，畳み込み層とプーリング層という二種類の階層を有していることが特徴であり，画像の特徴を際立たせ捉えることが可能である．最適化手法としてMomentumSDGを用い，過学習を防止するためweight decayを付加する．また，ニューラルネットワークの学習率をチューニングすることで，計算速度と収束のバランスが取れたネットワークを構築する．

\end{document}
